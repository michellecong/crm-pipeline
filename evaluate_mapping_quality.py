#!/usr/bin/env python3
"""
Mapping Quality Evaluation Script

è¯„ä¼° Mappings çš„è´¨é‡ï¼ŒåŒ…æ‹¬ï¼š
1. Value Proposition ä¸ Product çš„åŒ¹é…åº¦
2. Value Proposition ä¸ Persona çš„åŒ¹é…åº¦
3. æ–‡æœ¬è´¨é‡ï¼ˆé•¿åº¦ã€å®Œæ•´æ€§ï¼‰
4. é‡åŒ–æŒ‡æ ‡ï¼ˆæ˜¯å¦åŒ…å«é‡åŒ–æ”¶ç›Šï¼‰
5. Pain Point å’Œ Value Proposition çš„åŒ¹é…åº¦

æ··åˆè¯„ä¼°æ¨¡å¼ï¼š
- æ‰€æœ‰æŒ‡æ ‡ä½¿ç”¨ LLM è¯„ä¼°ï¼ˆä¸€æ¬¡è°ƒç”¨è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡ï¼‰
- ä¼ ç»Ÿæ–¹æ³•ä½œä¸ºè¡¥å……ï¼šText Qualityï¼ˆé•¿åº¦æ£€æŸ¥ï¼‰ã€Quantified Benefitsï¼ˆæ¨¡å¼åŒ¹é…ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluate_mapping_quality.py
"""
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

try:
    import pandas as pd
    HAS_PANDAS = True
except (ImportError, ValueError):
    HAS_PANDAS = False
    pd = None

# Try to import LLM service
try:
    import sys
    # Add project root to path to import app modules
    current_file = Path(__file__).absolute()
    project_root = current_file.parent  # evaluate_mapping_quality.py is in project root
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    from app.services.llm_service import LLMService
    HAS_LLM = True
except (ImportError, Exception):
    HAS_LLM = False
    # Only print warning if explicitly trying to use LLM
    pass


class MappingQualityEvaluator:
    """è¯„ä¼° Mapping è´¨é‡çš„ç±»ï¼ˆæ··åˆæ¨¡å¼ï¼šLLM + ä¼ ç»Ÿæ–¹æ³•è¡¥å……ï¼‰"""
    
    def __init__(self, evaluation_dir: Path):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            evaluation_dir: è¯„ä¼°æ•°æ®ç›®å½•
        """
        self.evaluation_dir = evaluation_dir
        
        # åˆå§‹åŒ– LLM æœåŠ¡
        self.use_llm = HAS_LLM
        self.llm_service = None
        if self.use_llm:
            try:
                print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– LLM æœåŠ¡...")
                self.llm_service = LLMService()
                print(f"âœ… LLM è¯„ä¼°æ¨¡å¼å·²å¯ç”¨ï¼ˆæ··åˆæ¨¡å¼ï¼šLLM + ä¼ ç»Ÿæ–¹æ³•è¡¥å……ï¼‰")
            except Exception as e:
                print(f"âš ï¸  LLM æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿè¯„ä¼°æ¨¡å¼")
                self.use_llm = False
        else:
            print(f"âš ï¸  LLM æœåŠ¡ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿè¯„ä¼°æ¨¡å¼")
        
        # è¡Œä¸šå…³é”®è¯æ˜ å°„
        self.industry_keywords = {
            "Financial Services": ["bank", "financial", "fraud", "compliance", "risk", "trading", "aml", "regulatory", "audit", "governance"],
            "Manufacturing": ["manufacturing", "plant", "factory", "production", "maintenance", "supply chain", "operations", "ot", "industrial"],
            "Healthcare": ["healthcare", "clinical", "patient", "medical", "pharma", "hospital", "ehr", "rcm", "revenue cycle"],
            "Retail/E-commerce": ["retail", "ecommerce", "merchandising", "inventory", "customer", "conversion", "sales", "store"],
            "B2B SaaS Platforms": ["saas", "revenue", "gtm", "revops", "sales ops", "forecast", "pipeline", "crm"],
        }
        
        # è§’è‰²å…³é”®è¯æ˜ å°„
        self.role_keywords = {
            "Data & AI": ["data", "analytics", "ml", "machine learning", "ai", "model", "pipeline", "governance", "catalog"],
            "Ops & Data Leaders": ["operations", "ops", "data", "analytics", "pipeline", "etl", "maintenance"],
            "Revenue Ops": ["revenue", "revops", "sales", "forecast", "pipeline", "gtm", "crm", "analytics"],
            "Merch & Analytics": ["merchandising", "analytics", "inventory", "customer", "personalization", "conversion"],
            "Clinical Data & RCM": ["clinical", "data", "rcm", "revenue cycle", "patient", "medical", "ehr"],
        }
        
        # å…¬å¸è§„æ¨¡å…³é”®è¯
        self.size_keywords = {
            "enterprise": ["enterprise", "large", "complex", "multi-site", "global", "scale"],
            "mid-market": ["mid-market", "growing", "scaling", "emerging"],
            "small": ["small", "startup", "lean", "boutique"]
        }
    
    def load_mappings_and_personas(self, company_name: str, architecture: str) -> Tuple[List[Dict], List[Dict], Optional[List[Dict]]]:
        """åŠ è½½æŸä¸ªå…¬å¸åœ¨æŸä¸ªæ¶æ„ä¸‹çš„ mappingsã€personas å’Œ products"""
        company_base_dir = self.evaluation_dir / company_name
        
        # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
        company_dir = company_base_dir / architecture
        if not company_dir.exists():
            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
            if company_base_dir.exists():
                for subdir in company_base_dir.iterdir():
                    if subdir.is_dir() and subdir.name.lower() == architecture.lower():
                        company_dir = subdir
                        break
                else:
                    return [], [], None
            else:
                return [], [], None
        
        mappings_data = []
        personas_data = []
        products_data = None
        
        # åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶
        for json_file in company_dir.glob("*.json"):
            filename = json_file.stem.lower()
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                
                if "mapping" in filename:
                    # Mappings æ–‡ä»¶
                    if "result" in content and "personas_with_mappings" in content["result"]:
                        mappings_data = content["result"]["personas_with_mappings"]
                elif "persona" in filename and "mapping" not in filename:
                    # ç‹¬ç«‹çš„ personas æ–‡ä»¶
                    if "result" in content and "personas" in content["result"]:
                        personas_data = content["result"]["personas"]
                elif "product" in filename:
                    # Products æ–‡ä»¶
                    if "result" in content and "products" in content["result"]:
                        products_data = content["result"]["products"]
                elif "two_stage" in filename:
                    # Two-Stage consolidated æ–‡ä»¶
                    if "result" in content:
                        if "personas_with_mappings" in content["result"]:
                            mappings_data = content["result"]["personas_with_mappings"]
                        if "personas" in content["result"]:
                            personas_data = content["result"]["personas"]
                        if "products" in content.get("result", {}):
                            products_data = content["result"]["products"]
                elif "three_stage" in filename:
                    # Three-Stage consolidated æ–‡ä»¶
                    if "result" in content:
                        if "personas_with_mappings" in content["result"]:
                            mappings_data = content["result"]["personas_with_mappings"]
                            # ä» mappings ä¸­æå– personas
                            for pwm in mappings_data:
                                persona = {
                                    "persona_name": pwm.get("persona_name"),
                                    "tier": pwm.get("tier"),
                                    "industry": pwm.get("industry"),
                                    "location": pwm.get("location"),
                                    "company_size_range": pwm.get("company_size_range"),
                                    "company_type": pwm.get("company_type"),
                                    "description": pwm.get("description"),
                                    "job_titles": pwm.get("job_titles", []),
                                }
                                if persona.get("persona_name"):
                                    personas_data.append(persona)
                            
            except Exception as e:
                print(f"Error loading {json_file}: {e}")
        
        return mappings_data, personas_data, products_data
    
    def evaluate_product_match(self, value_proposition: str, products: List[Dict]) -> Dict:
        """è¯„ä¼° Value Proposition ä¸ Product çš„åŒ¹é…åº¦"""
        if not products or len(products) == 0:
            return {
                "has_product_mention": False,
                "mentioned_products": [],
                "product_count": 0,
                "product_names_found": [],
                "score": 0.0
            }
        
        # æå–æ‰€æœ‰äº§å“åç§°
        product_names = [p.get("product_name", "") for p in products]
        
        # æ£€æŸ¥ Value Proposition ä¸­æ˜¯å¦æåŠäº§å“
        mentioned_products = []
        product_names_found = []
        
        value_prop_lower = value_proposition.lower()
        
        for product_name in product_names:
            if not product_name:
                continue
            
            # æ£€æŸ¥å®Œæ•´äº§å“åç§°
            if product_name.lower() in value_prop_lower:
                mentioned_products.append(product_name)
                product_names_found.append(product_name)
            else:
                # æ£€æŸ¥äº§å“åç§°çš„å…³é”®è¯ï¼ˆå»é™¤å…¬å¸åï¼‰
                product_keywords = self._extract_product_keywords(product_name)
                for keyword in product_keywords:
                    if keyword.lower() in value_prop_lower and len(keyword) > 3:
                        mentioned_products.append(product_name)
                        product_names_found.append(product_name)
                        break
        
        # è®¡ç®—åˆ†æ•°ï¼šæœ‰äº§å“æåŠå¾—1.0ï¼Œå¦åˆ™0.0
        score = 1.0 if len(mentioned_products) > 0 else 0.0
        
        return {
            "has_product_mention": len(mentioned_products) > 0,
            "mentioned_products": mentioned_products,
            "product_count": len(mentioned_products),
            "product_names_found": product_names_found,
            "score": score
        }
    
    def _extract_product_keywords(self, product_name: str) -> List[str]:
        """æå–äº§å“åç§°çš„å…³é”®è¯"""
        # ç§»é™¤å¸¸è§çš„å…¬å¸åå‰ç¼€
        name = product_name
        common_prefixes = ["databricks", "salesforce", "hubspot", "monday", "atlassian", "snowflake", "workday", "procore", "servicenow"]
        
        for prefix in common_prefixes:
            if name.lower().startswith(prefix.lower()):
                name = name[len(prefix):].strip()
                break
        
        # åˆ†å‰²æˆå…³é”®è¯
        keywords = re.split(r'[\s\-&/]+', name)
        keywords = [k for k in keywords if len(k) > 2]
        
        return keywords
    
    def evaluate_persona_match(self, value_proposition: str, pain_point: str, persona: Dict) -> Dict:
        """è¯„ä¼° Value Proposition ä¸ Persona çš„åŒ¹é…åº¦"""
        if not persona:
            return {
                "industry_match_score": 0.0,
                "role_match_score": 0.0,
                "size_match_score": 0.0,
                "overall_match_score": 0.0
            }
        
        # åˆå¹¶æ–‡æœ¬ç”¨äºå…³é”®è¯åŒ¹é…
        combined_text = (value_proposition + " " + pain_point).lower()
        
        # 1. è¡Œä¸šåŒ¹é…åº¦
        industry = persona.get("industry", "")
        industry_match_score = 0.0
        if industry:
            industry_keywords = self.industry_keywords.get(industry, [])
            if industry_keywords:
                matches = sum(1 for keyword in industry_keywords if keyword.lower() in combined_text)
                industry_match_score = min(matches / len(industry_keywords) * 2, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
        
        # 2. è§’è‰²åŒ¹é…åº¦ï¼ˆåŸºäº persona_name å’Œ job_titlesï¼‰
        role_match_score = 0.0
        persona_name = persona.get("persona_name", "").lower()
        
        # ä» persona_name æå–è§’è‰²å…³é”®è¯
        role_keywords_to_check = []
        for role, keywords in self.role_keywords.items():
            if any(kw in persona_name for kw in role.lower().split()):
                role_keywords_to_check.extend(keywords)
        
        # ä¹Ÿä» job_titles æå–å…³é”®è¯
        job_titles = persona.get("job_titles", [])
        for job_title in job_titles:
            job_lower = job_title.lower()
            if "data" in job_lower or "analytics" in job_lower:
                role_keywords_to_check.extend(["data", "analytics", "ml", "machine learning"])
            if "revenue" in job_lower or "sales" in job_lower:
                role_keywords_to_check.extend(["revenue", "sales", "forecast", "pipeline"])
            if "operations" in job_lower or "ops" in job_lower:
                role_keywords_to_check.extend(["operations", "ops", "pipeline", "etl"])
            if "merchandising" in job_lower or "commerce" in job_lower:
                role_keywords_to_check.extend(["merchandising", "inventory", "customer", "conversion"])
            if "clinical" in job_lower or "healthcare" in job_lower:
                role_keywords_to_check.extend(["clinical", "patient", "medical", "rcm"])
        
        if role_keywords_to_check:
            unique_keywords = list(set(role_keywords_to_check))
            matches = sum(1 for keyword in unique_keywords if keyword.lower() in combined_text)
            role_match_score = min(matches / max(len(unique_keywords), 1) * 2, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
        
        # 3. å…¬å¸è§„æ¨¡åŒ¹é…åº¦
        size_range = persona.get("company_size_range", "")
        size_match_score = 0.0
        if size_range:
            # åˆ¤æ–­æ˜¯ enterprise, mid-market è¿˜æ˜¯ small
            size_type = "enterprise"
            if "500" in size_range or "200" in size_range or "50" in size_range:
                if "2000" in size_range or "5000" in size_range or "10000" in size_range:
                    size_type = "mid-market"
                else:
                    size_type = "small"
            
            size_keywords = self.size_keywords.get(size_type, [])
            if size_keywords:
                matches = sum(1 for keyword in size_keywords if keyword.lower() in combined_text)
                size_match_score = min(matches / len(size_keywords) * 2, 1.0) if len(size_keywords) > 0 else 0.0
        
        # ç»¼åˆåŒ¹é…åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
        overall_match_score = (
            industry_match_score * 0.4 +
            role_match_score * 0.4 +
            size_match_score * 0.2
        )
        
        return {
            "industry_match_score": industry_match_score,
            "role_match_score": role_match_score,
            "size_match_score": size_match_score,
            "overall_match_score": overall_match_score,
            "industry": industry,
            "size_range": size_range
        }
    
    def evaluate_text_quality(self, pain_point: str, value_proposition: str) -> Dict:
        """è¯„ä¼°æ–‡æœ¬è´¨é‡"""
        pain_point_len = len(pain_point)
        value_prop_len = len(value_proposition)
        
        # æ£€æŸ¥é•¿åº¦æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼ˆ20-300å­—ç¬¦ï¼‰
        pain_point_valid = 20 <= pain_point_len <= 300
        value_prop_valid = 20 <= value_prop_len <= 300
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦ä¿¡æ¯
        pain_point_has_who = bool(re.search(r'\b(teams?|leaders?|engineers?|analysts?|reps?|staff|organizations?|companies?)\b', pain_point, re.I))
        pain_point_has_impact = bool(re.search(r'\b(causing|leading to|resulting in|increasing|decreasing|delaying|reducing|improving)\b', pain_point, re.I))
        
        value_prop_has_how = bool(re.search(r'\b(provides|enables|automates|unifies|consolidates|accelerates|reduces|improves|cuts|delivers)\b', value_proposition, re.I))
        
        completeness_score = (
            (1.0 if pain_point_has_who else 0.0) * 0.25 +
            (1.0 if pain_point_has_impact else 0.0) * 0.25 +
            (1.0 if value_prop_has_how else 0.0) * 0.5
        )
        
        return {
            "pain_point_length": pain_point_len,
            "value_proposition_length": value_prop_len,
            "pain_point_valid_length": pain_point_valid,
            "value_proposition_valid_length": value_prop_valid,
            "completeness_score": completeness_score,
            "pain_point_has_who": pain_point_has_who,
            "pain_point_has_impact": pain_point_has_impact,
            "value_prop_has_how": value_prop_has_how
        }
    
    def evaluate_quantified_benefits(self, value_proposition: str) -> Dict:
        """è¯„ä¼°é‡åŒ–æŒ‡æ ‡"""
        # æŸ¥æ‰¾ç™¾åˆ†æ¯”
        percentages = re.findall(r'\d+%', value_proposition)
        
        # æŸ¥æ‰¾å€æ•°ï¼ˆå¦‚ "3x", "10x"ï¼‰
        multipliers = re.findall(r'\d+x', value_proposition, re.I)
        
        # æŸ¥æ‰¾æ—¶é—´ï¼ˆå¦‚ "hours", "days", "weeks", "months"ï¼‰
        time_mentions = re.findall(r'\d+\s*(?:hour|day|week|month|year)', value_proposition, re.I)
        
        # æŸ¥æ‰¾é‡‘é¢ï¼ˆå¦‚ "$100K", "$2M"ï¼‰
        amounts = re.findall(r'\$[â‚¬Â£Â¥]?\s*\d+[KMB]?', value_proposition, re.I)
        
        has_quantified_benefit = len(percentages) > 0 or len(multipliers) > 0 or len(time_mentions) > 0 or len(amounts) > 0
        
        return {
            "has_quantified_benefit": has_quantified_benefit,
            "percentages": percentages,
            "multipliers": multipliers,
            "time_mentions": time_mentions,
            "amounts": amounts,
            "total_metrics": len(percentages) + len(multipliers) + len(time_mentions) + len(amounts)
        }
    
    def evaluate_all_metrics_with_llm(
        self,
        pain_point: str,
        value_proposition: str,
        persona: Dict,
        products: List[Dict]
    ) -> Optional[Dict]:
        """
        ä¸€æ¬¡ LLM è°ƒç”¨è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡ï¼ˆæ··åˆæ¨¡å¼ï¼‰
        
        Args:
            pain_point: Pain Point æ–‡æœ¬
            value_proposition: Value Proposition æ–‡æœ¬
            persona: Persona æ•°æ®
            products: äº§å“åˆ—è¡¨
        
        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡è¯„ä¼°ç»“æœçš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        if not self.use_llm:
            return None
        
        # æ„å»ºè¯„ä¼° prompt
        persona_desc = persona.get('description', 'N/A') or 'N/A'
        if persona_desc != 'N/A' and len(persona_desc) > 300:
            persona_desc = persona_desc[:300] + "..."
        
        persona_info = f"""
Persona Name: {persona.get('persona_name', 'N/A')}
Industry: {persona.get('industry', 'N/A')}
Company Size: {persona.get('company_size_range', 'N/A')}
Job Titles: {', '.join(persona.get('job_titles', []) or [])}
Description: {persona_desc}
"""
        
        products_info = ""
        if products and len(products) > 0:
            products_list = "\n".join([
                f"- {p.get('product_name', 'N/A')}: {p.get('description', 'N/A')[:150]}"
                for p in products[:10]
            ])
            products_info = f"""
## äº§å“åˆ—è¡¨
{products_list}
"""
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ Pain Point å’Œ Value Proposition çš„åŒ¹é…è´¨é‡ã€‚

## Persona ä¿¡æ¯
{persona_info}{products_info}
## å¾…è¯„ä¼°çš„ Mapping
Pain Point: {pain_point}
Value Proposition: {value_proposition}

## è¯„ä¼°ä»»åŠ¡
è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼š

1. **Pain-Value Match (é—®é¢˜-æ–¹æ¡ˆåŒ¹é…åº¦)**: Value Proposition æ˜¯å¦ç›´æ¥ã€æœ‰æ•ˆåœ°è§£å†³äº† Pain Point ä¸­æåˆ°çš„é—®é¢˜ï¼Ÿéœ€è¦æ·±åº¦è¯­ä¹‰ç†è§£ï¼Œåˆ¤æ–­æ–¹æ¡ˆæ˜¯å¦çœŸæ­£è§£å†³é—®é¢˜ã€‚

2. **Persona Match (è§’è‰²åŒ¹é…åº¦)**: Value Proposition æ˜¯å¦ä¸ Persona çš„è§’è‰²ã€è¡Œä¸šã€å…¬å¸è§„æ¨¡ç›¸åŒ¹é…ï¼Ÿéœ€è¦ç†è§£éšå«çš„è§’è‰²ç‰¹å¾å’Œè¡Œä¸šèƒŒæ™¯ã€‚

3. **Product Match (äº§å“åŒ¹é…åº¦)**: Value Proposition æ˜¯å¦è‡ªç„¶ã€åˆç†åœ°æåŠäº†ç›¸å…³äº§å“ï¼Ÿèƒ½å¦ç†è§£æ¦‚å¿µåŒ¹é…ï¼ˆå¦‚ 'unified analytics platform' = 'Lakehouse'ï¼‰ï¼Ÿ

4. **Text Quality (æ–‡æœ¬æµç•…åº¦)**: æ–‡æœ¬æ˜¯å¦æµç•…ã€è‡ªç„¶ï¼Œæ²¡æœ‰è¯­æ³•é”™è¯¯ï¼Ÿæ˜¯å¦ç¬¦åˆ B2B SaaS è¡Œä¸šçš„ä¸“ä¸šè¡¨è¾¾ï¼Ÿè¯­æ°”æ˜¯å¦åˆé€‚ï¼Ÿ

è¯·ä»¥ JSON æ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
  "pain_value_match": {{
    "score": 0.0-1.0,
    "reason": "è¯¦ç»†è¯´æ˜åŒ¹é…åº¦çš„ç†ç”±"
  }},
  "persona_match": {{
    "overall_match_score": 0.0-1.0,
    "role_match_score": 0.0-1.0,
    "industry_match_score": 0.0-1.0,
    "size_match_score": 0.0-1.0,
    "reason": "è¯¦ç»†è¯´æ˜åŒ¹é…åº¦çš„ç†ç”±"
  }},
  "product_match": {{
    "score": 0.0-1.0,
    "has_product_mention": true/false,
    "mentioned_products": ["product1", "product2", ...],
    "reason": "è¯¦ç»†è¯´æ˜åŒ¹é…åº¦çš„ç†ç”±"
  }},
  "text_quality": {{
    "fluency_score": 0.0-1.0,
    "professionalism_score": 0.0-1.0,
    "overall_score": 0.0-1.0,
    "reason": "è¯¦ç»†è¯´æ˜è¯„ä¼°ç†ç”±"
  }}
}}
"""
        
        try:
            response = self.llm_service.generate(
                prompt=prompt,
                system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ B2B è¥é”€å†…å®¹è¯„ä¼°ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æå¹¶ç»™å‡ºå®¢è§‚ã€ä¸“ä¸šçš„è¯„ä¼°ã€‚",
                temperature=None,
                max_completion_tokens=1500
            )
            
            # è§£æ JSON å“åº”
            content = response.content.strip()
            
            if not content:
                raise ValueError("LLM è¿”å›äº†ç©ºå“åº”")
            
            # æå– JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                parts = content.split("```")
                if len(parts) >= 2:
                    content = parts[1].strip()
                    if content.startswith("json"):
                        content = content[4:].strip()
                    elif content.startswith("JSON"):
                        content = content[4:].strip()
            
            # è§£æ JSON
            try:
                result = json.loads(content)
            except json.JSONDecodeError as e:
                import re
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group(0))
                    except json.JSONDecodeError:
                        return None
                else:
                    return None
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            llm_results = {}
            
            if 'pain_value_match' in result:
                pvm = result["pain_value_match"]
                llm_results['pain_value_match'] = {
                    "match_score": float(pvm.get("score", 0.0)),
                    "reason": pvm.get("reason", ""),
                    "evaluation_method": "llm"
                }
            
            if 'persona_match' in result:
                pm = result["persona_match"]
                llm_results['persona_match'] = {
                    "overall_match_score": float(pm.get("overall_match_score", 0.0)),
                    "role_match_score": float(pm.get("role_match_score", 0.0)),
                    "industry_match_score": float(pm.get("industry_match_score", 0.0)),
                    "size_match_score": float(pm.get("size_match_score", 0.0)),
                    "reason": pm.get("reason", ""),
                    "evaluation_method": "llm"
                }
            
            if 'product_match' in result:
                pm = result["product_match"]
                llm_results['product_match'] = {
                    "score": float(pm.get("score", 0.0)),
                    "has_product_mention": bool(pm.get("has_product_mention", False)),
                    "mentioned_products": pm.get("mentioned_products", []),
                    "reason": pm.get("reason", ""),
                    "evaluation_method": "llm"
                }
            
            if 'text_quality' in result:
                tq = result["text_quality"]
                llm_results['text_quality'] = {
                    "fluency_score": float(tq.get("fluency_score", 0.0)),
                    "professionalism_score": float(tq.get("professionalism_score", 0.0)),
                    "overall_score": float(tq.get("overall_score", 0.0)),
                    "reason": tq.get("reason", ""),
                    "evaluation_method": "llm"
                }
            
            return llm_results
            
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œè¿”å› Noneï¼Œè®©è°ƒç”¨è€…å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            # åªåœ¨è°ƒè¯•æ—¶æ‰“å°é”™è¯¯
            if False:  # è®¾ç½®ä¸º True å¯ä»¥çœ‹åˆ°è¯¦ç»†é”™è¯¯
                print(f"    LLM è¯„ä¼°å¼‚å¸¸: {e}")
            return None
    
    def evaluate_with_llm(
        self,
        pain_point: str,
        value_proposition: str,
        persona: Dict,
        products: List[Dict]
    ) -> Dict:
        """ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½è¯„ä¼°"""
        if not self.use_llm:
            raise ValueError("LLM evaluation is not enabled")
        
        # æ„å»ºè¯„ä¼° prompt
        persona_desc = persona.get('description', 'N/A') or 'N/A'
        if persona_desc != 'N/A' and len(persona_desc) > 200:
            persona_desc = persona_desc[:200]
        
        persona_info = f"""
Persona Name: {persona.get('persona_name', 'N/A')}
Industry: {persona.get('industry', 'N/A')}
Company Size: {persona.get('company_size_range', 'N/A')}
Job Titles: {', '.join(persona.get('job_titles', []) or [])}
Description: {persona_desc}
"""
        
        products_info = "\n".join([
            f"- {p.get('product_name', 'N/A')}: {p.get('description', 'N/A')[:100]}"
            for p in (products or [])[:5]
        ])
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ Pain Point å’Œ Value Proposition çš„åŒ¹é…è´¨é‡ã€‚

## Persona ä¿¡æ¯
{persona_info}

## äº§å“åˆ—è¡¨
{products_info}

## å¾…è¯„ä¼°çš„ Mapping
Pain Point: {pain_point}
Value Proposition: {value_proposition}

## è¯„ä¼°ä»»åŠ¡
è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œæ¯ä¸ªç»´åº¦ç»™å‡º 0-1 çš„åˆ†æ•°å’Œç®€çŸ­ç†ç”±ï¼š

1. **Product Match (äº§å“åŒ¹é…åº¦)**: Value Proposition æ˜¯å¦è‡ªç„¶ã€åˆç†åœ°æåŠäº†ç›¸å…³äº§å“ï¼Ÿäº§å“åç§°æ˜¯å¦ä¸äº§å“åˆ—è¡¨åŒ¹é…ï¼Ÿ
2. **Persona Match (è§’è‰²åŒ¹é…åº¦)**: Value Proposition æ˜¯å¦ä¸ Persona çš„è§’è‰²ã€è¡Œä¸šã€å…¬å¸è§„æ¨¡ç›¸åŒ¹é…ï¼Ÿ
3. **Pain-Value Match (é—®é¢˜-æ–¹æ¡ˆåŒ¹é…åº¦)**: Value Proposition æ˜¯å¦ç›´æ¥ã€æœ‰æ•ˆåœ°è§£å†³äº† Pain Point ä¸­æåˆ°çš„é—®é¢˜ï¼Ÿ
4. **Text Quality (æ–‡æœ¬è´¨é‡)**: æ–‡æœ¬æ˜¯å¦æ¸…æ™°ã€å®Œæ•´ã€ä¸“ä¸šï¼Ÿé•¿åº¦æ˜¯å¦åˆé€‚ï¼ˆ20-300å­—ç¬¦ï¼‰ï¼Ÿ
5. **Quantified Benefits (é‡åŒ–æ”¶ç›Š)**: æ˜¯å¦åŒ…å«å…·ä½“çš„é‡åŒ–æŒ‡æ ‡ï¼ˆç™¾åˆ†æ¯”ã€å€æ•°ã€æ—¶é—´ã€é‡‘é¢ï¼‰ï¼Ÿ

è¯·ä»¥ JSON æ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
  "product_match": {{
    "score": 0.0-1.0,
    "reason": "ç®€çŸ­ç†ç”±"
  }},
  "persona_match": {{
    "score": 0.0-1.0,
    "reason": "ç®€çŸ­ç†ç”±"
  }},
  "pain_value_match": {{
    "score": 0.0-1.0,
    "reason": "ç®€çŸ­ç†ç”±"
  }},
  "text_quality": {{
    "score": 0.0-1.0,
    "reason": "ç®€çŸ­ç†ç”±"
  }},
  "quantified_benefits": {{
    "score": 0.0-1.0,
    "has_quantified": true/false,
    "reason": "ç®€çŸ­ç†ç”±"
  }},
  "overall_score": 0.0-1.0,
  "overall_reason": "æ€»ä½“è¯„ä»·"
}}
"""
        
        try:
            # è°ƒç”¨ LLM
            # æ³¨æ„ï¼šæŸäº›æ¨¡å‹ï¼ˆå¦‚ gpt-5-miniï¼‰å¯èƒ½åªæ”¯æŒé»˜è®¤ temperatureï¼Œæ‰€ä»¥ä½¿ç”¨ None è®©æœåŠ¡ä½¿ç”¨é»˜è®¤å€¼
            response = self.llm_service.generate(
                prompt=prompt,
                system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ B2B è¥é”€å†…å®¹è¯„ä¼°ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æå¹¶ç»™å‡ºå®¢è§‚ã€ä¸“ä¸šçš„è¯„ä¼°ã€‚",
                temperature=None,  # ä½¿ç”¨é»˜è®¤å€¼ï¼Œè®©æ¨¡å‹å†³å®š
                max_completion_tokens=1000
            )
            
            # è§£æ JSON å“åº”
            content = response.content.strip()
            
            # å¦‚æœå“åº”ä¸ºç©ºï¼ŒæŠ›å‡ºå¼‚å¸¸
            if not content:
                raise ValueError("LLM è¿”å›äº†ç©ºå“åº”")
            
            # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å« markdown ä»£ç å—ï¼‰
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªä»£ç å—
                parts = content.split("```")
                if len(parts) >= 2:
                    content = parts[1].strip()
                    # ç§»é™¤è¯­è¨€æ ‡è¯†ç¬¦ï¼ˆå¦‚ "json"ï¼‰
                    if content.startswith("json"):
                        content = content[4:].strip()
                    elif content.startswith("JSON"):
                        content = content[4:].strip()
            
            # å°è¯•è§£æ JSON
            try:
                result = json.loads(content)
            except json.JSONDecodeError as e:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾ JSON å¯¹è±¡
                import re
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group(0))
                    except json.JSONDecodeError:
                        raise ValueError(f"æ— æ³•è§£æ LLM è¿”å›çš„ JSON: {e}. å“åº”å†…å®¹: {content[:200]}")
                else:
                    raise ValueError(f"æ— æ³•è§£æ LLM è¿”å›çš„ JSON: {e}. å“åº”å†…å®¹: {content[:200]}")
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            return {
                "product_match": {
                    "score": result.get("product_match", {}).get("score", 0.0),
                    "has_product_mention": result.get("product_match", {}).get("score", 0.0) > 0.5,
                    "reason": result.get("product_match", {}).get("reason", "")
                },
                "persona_match": {
                    "overall_match_score": result.get("persona_match", {}).get("score", 0.0),
                    "reason": result.get("persona_match", {}).get("reason", "")
                },
                "text_quality": {
                    "completeness_score": result.get("text_quality", {}).get("score", 0.0),
                    "reason": result.get("text_quality", {}).get("reason", "")
                },
                "quantified_benefits": {
                    "has_quantified_benefit": result.get("quantified_benefits", {}).get("has_quantified", False),
                    "score": result.get("quantified_benefits", {}).get("score", 0.0),
                    "reason": result.get("quantified_benefits", {}).get("reason", "")
                },
                "pain_value_match": {
                    "match_score": result.get("pain_value_match", {}).get("score", 0.0),
                    "reason": result.get("pain_value_match", {}).get("reason", "")
                },
                "overall_score": result.get("overall_score", 0.0),
                "overall_reason": result.get("overall_reason", ""),
                "evaluation_method": "llm"
            }
            
        except Exception as e:
            print(f"âš ï¸  LLM è¯„ä¼°å¤±è´¥: {e}ï¼Œå›é€€åˆ°ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            return None
    
    def evaluate_pain_value_match(self, pain_point: str, value_proposition: str) -> Dict:
        """è¯„ä¼° Pain Point å’Œ Value Proposition çš„åŒ¹é…åº¦"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        pain_lower = pain_point.lower()
        value_lower = value_proposition.lower()
        
        # æå– Pain Point ä¸­çš„é—®é¢˜å…³é”®è¯
        problem_keywords = []
        problem_patterns = [
            r"struggle\s+with\s+(\w+)",
            r"lack\s+(\w+)",
            r"can't\s+(\w+)",
            r"waste\s+(\w+)",
            r"face\s+(\w+)",
            r"spend\s+(\w+)",
        ]
        
        for pattern in problem_patterns:
            matches = re.findall(pattern, pain_lower)
            problem_keywords.extend(matches)
        
        # æå– Value Proposition ä¸­çš„è§£å†³æ–¹æ¡ˆå…³é”®è¯
        solution_keywords = []
        solution_patterns = [
            r"provides\s+(\w+)",
            r"enables\s+(\w+)",
            r"automates\s+(\w+)",
            r"unifies\s+(\w+)",
            r"consolidates\s+(\w+)",
            r"reduces\s+(\w+)",
            r"improves\s+(\w+)",
        ]
        
        for pattern in solution_patterns:
            matches = re.findall(pattern, value_lower)
            solution_keywords.extend(matches)
        
        # è®¡ç®—åŒ¹é…åº¦ï¼ˆç®€å•çš„å…³é”®è¯é‡å ï¼‰
        if problem_keywords and solution_keywords:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„å…³é”®è¯åŒ¹é…
            common_keywords = set(["data", "analytics", "pipeline", "revenue", "sales"])
            matches = sum(
                1 for pk in problem_keywords[:3] for sk in solution_keywords[:3]
                if pk in sk or sk in pk or len(set([pk, sk]) & common_keywords) > 0
            )
            match_score = min(
                matches / max(len(problem_keywords), len(solution_keywords)), 1.0
            )
        else:
            match_score = 0.5  # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„å…³é”®è¯ï¼Œç»™ä¸­ç­‰åˆ†æ•°
        
        return {
            "match_score": match_score,
            "problem_keywords": problem_keywords[:5],
            "solution_keywords": solution_keywords[:5]
        }
    
    def evaluate_all_mappings(self, company_name: str, architecture: str) -> Dict:
        """è¯„ä¼°æŸä¸ªå…¬å¸åœ¨æŸä¸ªæ¶æ„ä¸‹çš„æ‰€æœ‰ mappings"""
        mappings_data, personas_data, products_data = self.load_mappings_and_personas(
            company_name, architecture
        )
        
        if not mappings_data:
            return {
                "company_name": company_name,
                "architecture": architecture,
                "error": "No mappings data found"
            }
        
        # åˆ›å»º persona å­—å…¸ï¼ˆé€šè¿‡ persona_name ç´¢å¼•ï¼‰
        persona_dict = {p.get("persona_name", ""): p for p in personas_data}
        
        results = {
            "company_name": company_name,
            "architecture": architecture,
            "total_personas": len(mappings_data),
            "total_mappings": 0,
            "mapping_details": []
        }
        
        total_mappings_count = sum(len(pm.get("mappings", [])) for pm in mappings_data)
        print(f"  æ‰¾åˆ° {total_mappings_count} ä¸ª mappingsï¼Œå¼€å§‹è¯„ä¼°...")
        
        for idx, persona_mapping in enumerate(mappings_data):
            persona_name = persona_mapping.get("persona_name", "")
            mappings = persona_mapping.get("mappings", [])
            
            persona = persona_dict.get(persona_name, {})
            print(f"  è¯„ä¼° Persona: {persona_name} ({len(mappings)} ä¸ª mappings)")
            
            for mapping_idx, mapping in enumerate(mappings):
                pain_point = mapping.get("pain_point", "")
                value_proposition = mapping.get("value_proposition", "")
                
                # æ˜¾ç¤ºè¿›åº¦
                current_count = results["total_mappings"] + 1
                if current_count % 5 == 0 or current_count == 1:
                    print(f"    æ­£åœ¨è¯„ä¼°ç¬¬ {current_count}/{total_mappings_count} ä¸ª mapping...")
                
                # æ··åˆè¯„ä¼°ï¼šä¸€æ¬¡ LLM è°ƒç”¨è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡
                evaluation_metadata = {"method": "hybrid"}
                
                # ä¸€æ¬¡ LLM è°ƒç”¨è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡
                try:
                    llm_results = self.evaluate_all_metrics_with_llm(
                        pain_point, value_proposition, persona, products_data or []
                    )
                except Exception as e:
                    print(f"    âš ï¸  LLM è°ƒç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                    llm_results = None
                
                # 1. Pain-Value Match
                if llm_results and 'pain_value_match' in llm_results:
                    pain_value_match = llm_results['pain_value_match']
                else:
                    pain_value_match = self.evaluate_pain_value_match(pain_point, value_proposition)
                    pain_value_match["evaluation_method"] = "traditional"
                
                # 2. Persona Match
                if llm_results and 'persona_match' in llm_results:
                    persona_match = llm_results['persona_match']
                else:
                    persona_match = self.evaluate_persona_match(value_proposition, pain_point, persona)
                    persona_match["evaluation_method"] = "traditional"
                
                # 3. Product Match
                if llm_results and 'product_match' in llm_results:
                    product_match = llm_results['product_match']
                else:
                    product_match = self.evaluate_product_match(value_proposition, products_data or [])
                    product_match["evaluation_method"] = "traditional"
                
                # 4. Text Qualityï¼ˆæ··åˆï¼šä¼ ç»Ÿæ–¹æ³•æ£€æŸ¥é•¿åº¦ + LLM è¯„ä¼°æµç•…åº¦ï¼‰
                text_quality = self.evaluate_text_quality(pain_point, value_proposition)  # ä¼ ç»Ÿæ–¹æ³•ï¼šé•¿åº¦ã€ç»“æ„
                text_quality["evaluation_method"] = "traditional"
                
                if llm_results and 'text_quality' in llm_results:
                    tq_llm = llm_results['text_quality']
                    text_quality["fluency_score"] = tq_llm.get("fluency_score", 0.0)
                    text_quality["professionalism_score"] = tq_llm.get("professionalism_score", 0.0)
                    text_quality["fluency_reason"] = tq_llm.get("reason", "")
                    # ç»¼åˆåˆ†æ•°ï¼šåŸºç¡€æ£€æŸ¥ï¼ˆ50%ï¼‰+ æµç•…åº¦ï¼ˆ50%ï¼‰
                    text_quality["completeness_score"] = (
                        text_quality["completeness_score"] * 0.5 +
                        tq_llm.get("overall_score", 0.0) * 0.5
                    )
                    text_quality["evaluation_method"] = "hybrid"
                
                # 5. Quantified Benefitsï¼ˆä¼ ç»Ÿæ–¹æ³•è¶³å¤Ÿï¼Œæ¨¡å¼åŒ¹é…ä»»åŠ¡ï¼‰
                quantified_benefits = self.evaluate_quantified_benefits(value_proposition)
                quantified_benefits["evaluation_method"] = "traditional"
                
                results["mapping_details"].append({
                    "persona_name": persona_name,
                    "pain_point": pain_point,
                    "value_proposition": value_proposition,
                    "product_match": product_match,
                    "persona_match": persona_match,
                    "text_quality": text_quality,
                    "quantified_benefits": quantified_benefits,
                    "pain_value_match": pain_value_match,
                    "evaluation_metadata": evaluation_metadata
                })
                
                results["total_mappings"] += 1
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        if results["mapping_details"]:
            product_match_scores = [m["product_match"]["score"] for m in results["mapping_details"]]
            persona_match_scores = [m["persona_match"]["overall_match_score"] for m in results["mapping_details"]]
            text_quality_scores = [m["text_quality"]["completeness_score"] for m in results["mapping_details"]]
            quantified_benefit_rates = [1.0 if m["quantified_benefits"]["has_quantified_benefit"] else 0.0 for m in results["mapping_details"]]
            pain_value_match_scores = [m["pain_value_match"]["match_score"] for m in results["mapping_details"]]
            
            results["summary"] = {
                "avg_product_match_score": sum(product_match_scores) / len(product_match_scores) if product_match_scores else 0.0,
                "avg_persona_match_score": sum(persona_match_scores) / len(persona_match_scores) if persona_match_scores else 0.0,
                "avg_text_quality_score": sum(text_quality_scores) / len(text_quality_scores) if text_quality_scores else 0.0,
                "quantified_benefit_rate": sum(quantified_benefit_rates) / len(quantified_benefit_rates) if quantified_benefit_rates else 0.0,
                "avg_pain_value_match_score": sum(pain_value_match_scores) / len(pain_value_match_scores) if pain_value_match_scores else 0.0,
                "mappings_with_product_mention": sum(1 for m in results["mapping_details"] if m["product_match"]["has_product_mention"]),
                "mappings_with_quantified_benefits": sum(1 for m in results["mapping_details"] if m["quantified_benefits"]["has_quantified_benefit"])
            }
        
        return results
    
    def compare_architectures(self, two_stage_results: Dict, three_stage_results: Dict = None, four_stage_results: Dict = None) -> Dict:
        """å¯¹æ¯”ä¸¤ç§æˆ–ä¸‰ç§æ¶æ„çš„ mappings è´¨é‡"""
        comparison = {
            "company_name": two_stage_results.get("company_name", ""),
            "comparison": {}
        }
        
        two_summary = two_stage_results.get("summary", {})
        three_summary = three_stage_results.get("summary", {}) if three_stage_results else {}
        four_summary = four_stage_results.get("summary", {}) if four_stage_results else {}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        has_two = bool(two_summary)
        has_three = bool(three_summary)
        has_four = bool(four_summary)
        
        if not has_two:
            comparison["comparison"]["error"] = "Missing 2-stage summary data"
            return comparison
        
        if not has_three and not has_four:
            comparison["comparison"]["error"] = "Missing 3-stage or 4-stage summary data"
            return comparison
        
        # å¯¹æ¯”å„é¡¹æŒ‡æ ‡
        metrics = {
            "product_match_score": "avg_product_match_score",
            "persona_match_score": "avg_persona_match_score",
            "text_quality_score": "avg_text_quality_score",
            "quantified_benefit_rate": "quantified_benefit_rate",
            "pain_value_match_score": "avg_pain_value_match_score"
        }
        
        comparison_details = {}
        for metric_name, summary_key in metrics.items():
            two_value = two_summary.get(summary_key, 0)
            three_value = three_summary.get(summary_key, 0) if has_three else None
            four_value = four_summary.get(summary_key, 0) if has_four else None
            
            # æ‰¾å‡ºæœ€ä½³å€¼
            values = {"two_stage": two_value}
            if three_value is not None:
                values["three_stage"] = three_value
            if four_value is not None:
                values["four_stage"] = four_value
            
            best_stage = max(values.items(), key=lambda x: x[1])[0] if values else "two_stage"
            
            comparison_details[metric_name] = {
                "two_stage": two_value,
                "three_stage": three_value,
                "four_stage": four_value,
                "best": best_stage
            }
        
        # äº§å“æåŠç‡å¯¹æ¯”
        two_product_mention_rate = two_stage_results.get("summary", {}).get("mappings_with_product_mention", 0) / max(two_stage_results.get("total_mappings", 1), 1)
        three_product_mention_rate = None
        four_product_mention_rate = None
        
        if has_three:
            three_product_mention_rate = three_stage_results.get("summary", {}).get("mappings_with_product_mention", 0) / max(three_stage_results.get("total_mappings", 1), 1)
        if has_four:
            four_product_mention_rate = four_stage_results.get("summary", {}).get("mappings_with_product_mention", 0) / max(four_stage_results.get("total_mappings", 1), 1)
        
        mention_values = {"two_stage": two_product_mention_rate}
        if three_product_mention_rate is not None:
            mention_values["three_stage"] = three_product_mention_rate
        if four_product_mention_rate is not None:
            mention_values["four_stage"] = four_product_mention_rate
        
        best_mention_stage = max(mention_values.items(), key=lambda x: x[1])[0] if mention_values else "two_stage"
        
        comparison_details["product_mention_rate"] = {
            "two_stage": two_product_mention_rate,
            "three_stage": three_product_mention_rate,
            "four_stage": four_product_mention_rate,
            "best": best_mention_stage
        }
        
        comparison["comparison"] = comparison_details
        
        return comparison


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼° Mappings è´¨é‡ï¼ˆæ··åˆæ¨¡å¼ï¼‰")
    parser.add_argument(
        "--company",
        type=str,
        help="åªè¯„ä¼°æŒ‡å®šçš„å…¬å¸ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œè¯„ä¼°æ‰€æœ‰å…¬å¸ï¼‰"
    )
    args = parser.parse_args()
    
    evaluation_dir = Path("data/Evaluation")
    
    if not evaluation_dir.exists():
        print(f"âŒ è¯„ä¼°ç›®å½•ä¸å­˜åœ¨: {evaluation_dir}")
        return
    
    evaluator = MappingQualityEvaluator(evaluation_dir)
    
    # è·å–è¦è¯„ä¼°çš„å…¬å¸åˆ—è¡¨
    if args.company:
        companies = [args.company]
        if not (evaluation_dir / args.company).exists():
            print(f"âŒ å…¬å¸ç›®å½•ä¸å­˜åœ¨: {args.company}")
            return
    else:
        companies = [d.name for d in evaluation_dir.iterdir() if d.is_dir()]
    
    if not companies:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å…¬å¸æ•°æ®")
        return
    
    print(f"ğŸš€ å¼€å§‹è¯„ä¼° Mappings è´¨é‡...")
    print(f"ğŸ“ è¯„ä¼°ç›®å½•: {evaluation_dir}")
    print(f"ğŸ“Š è¯„ä¼° {len(companies)} ä¸ªå…¬å¸: {', '.join(companies)}")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = Path("evaluation_results")
    output_dir.mkdir(exist_ok=True)
    
    all_results = []
    all_comparisons = []
    
    for idx, company_name in enumerate(companies, 1):
        print(f"\n{'='*80}")
        print(f"è¯„ä¼°å…¬å¸ {idx}/{len(companies)}: {company_name}")
        print(f"{'='*80}")
        
        try:
            # è¯„ä¼° 2 Stage
            print(f"\nğŸ“Š è¯„ä¼° 2 Stage...")
            two_stage_results = evaluator.evaluate_all_mappings(company_name, "2 Stage")
            if "error" in two_stage_results:
                two_stage_results = evaluator.evaluate_all_mappings(company_name, "Two-Stage")
            if "error" in two_stage_results:
                two_stage_results = evaluator.evaluate_all_mappings(company_name, "2 stage")
            
            # è¯„ä¼° 3 Stage
            print(f"\nğŸ“Š è¯„ä¼° 3 Stage...")
            three_stage_results = evaluator.evaluate_all_mappings(company_name, "3 Stage")
            if "error" in three_stage_results:
                three_stage_results = evaluator.evaluate_all_mappings(company_name, "Three-Stage")
            if "error" in three_stage_results:
                three_stage_results = evaluator.evaluate_all_mappings(company_name, "3 stage")
            
            # è¯„ä¼° 4 Stage
            print(f"\nğŸ“Š è¯„ä¼° 4 Stage...")
            four_stage_results = evaluator.evaluate_all_mappings(company_name, "4 Stage")
            if "error" in four_stage_results:
                four_stage_results = evaluator.evaluate_all_mappings(company_name, "Four-Stage")
            if "error" in four_stage_results:
                four_stage_results = evaluator.evaluate_all_mappings(company_name, "4 stage")
            
            # è¿›è¡Œä¸‰æ–¹æ¯”è¾ƒ
            if "error" not in two_stage_results:
                comparison = evaluator.compare_architectures(
                    two_stage_results, 
                    three_stage_results if "error" not in three_stage_results else None,
                    four_stage_results if "error" not in four_stage_results else None
                )
                if "error" not in comparison.get("comparison", {}):
                    all_comparisons.append(comparison)
            
            all_results.append({
                "company_name": company_name,
                "two_stage": two_stage_results,
                "three_stage": three_stage_results if "error" not in three_stage_results else None,
                "four_stage": four_stage_results if "error" not in four_stage_results else None
            })
            
            print(f"\nâœ… {company_name} è¯„ä¼°å®Œæˆ ({idx}/{len(companies)})")
            
            # æ¯è¯„ä¼°å®Œä¸€ä¸ªå…¬å¸å°±ä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢ä¸­é€”å‡ºé”™ä¸¢å¤±æ•°æ®ï¼‰
            if idx % 2 == 0 or idx == len(companies):
                temp_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                temp_file = output_dir / f"mapping_quality_evaluation_temp_{temp_timestamp}.json"
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(all_results, f, indent=2, ensure_ascii=False)
                print(f"ğŸ’¾ ä¸´æ—¶ä¿å­˜: {temp_file} ({len(all_results)} ä¸ªå…¬å¸)")
                
        except Exception as e:
            print(f"\nâŒ è¯„ä¼° {company_name} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜å·²å®Œæˆçš„è¯„ä¼°
            all_results.append({
                "company_name": company_name,
                "error": str(e)
            })
            continue
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"\n{'='*80}")
    print(f"å¼€å§‹ä¿å­˜æœ€ç»ˆç»“æœï¼ˆå…± {len(all_results)} ä¸ªå…¬å¸ï¼‰...")
    print(f"{'='*80}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = output_dir / f"mapping_quality_evaluation_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_file = output_dir / f"mapping_quality_comparison_{timestamp}.json"
    with open(comparison_file, 'w', encoding='utf-8') as f:
        json.dump(all_comparisons, f, indent=2, ensure_ascii=False)
    print(f"âœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {comparison_file}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print_summary(all_comparisons)
    
    print(f"\nâœ¨ è¯„ä¼°å®Œæˆï¼")
    print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    # ç¡®ä¿ç¨‹åºæ­£å¸¸é€€å‡ºï¼ˆæ¸…ç†èµ„æºï¼‰
    import sys
    sys.exit(0)


def print_summary(comparisons: List[Dict]):
    """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("Mappings è´¨é‡è¯„ä¼°æ±‡æ€»ï¼ˆ2 Stage vs 3 Stage vs 4 Stageï¼‰")
    print("=" * 80)
    
    for comparison in comparisons:
        company_name = comparison["company_name"]
        comp = comparison.get("comparison", {})
        
        if "error" in comp:
            print(f"\nâš ï¸  {company_name}: {comp['error']}")
            continue
        
        print(f"\nğŸ“Š {company_name}:")
        print("-" * 80)
        
        for metric_name, metric_data in comp.items():
            if isinstance(metric_data, dict) and "two_stage" in metric_data:
                two_val = metric_data["two_stage"]
                three_val = metric_data.get("three_stage")
                four_val = metric_data.get("four_stage")
                best = metric_data.get("best", "two_stage")
                
                print(f"\n{metric_name}:")
                print(f"  2 Stage: {two_val:.3f}")
                if three_val is not None:
                    marker_3 = " â­" if best == "three_stage" else ""
                    print(f"  3 Stage: {three_val:.3f}{marker_3}")
                if four_val is not None:
                    marker_4 = " â­" if best == "four_stage" else ""
                    print(f"  4 Stage: {four_val:.3f}{marker_4}")
                print(f"  æœ€ä½³: {best.replace('_', ' ').title()}")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    if comparisons:
        metrics_to_avg = {}
        for comparison in comparisons:
            comp = comparison.get("comparison", {})
            if "error" in comp:
                continue
            
            for metric_name, metric_data in comp.items():
                if isinstance(metric_data, dict) and "two_stage" in metric_data:
                    if metric_name not in metrics_to_avg:
                        metrics_to_avg[metric_name] = {"two": [], "three": [], "four": []}
                    metrics_to_avg[metric_name]["two"].append(metric_data["two_stage"])
                    if metric_data.get("three_stage") is not None:
                        metrics_to_avg[metric_name]["three"].append(metric_data["three_stage"])
                    if metric_data.get("four_stage") is not None:
                        metrics_to_avg[metric_name]["four"].append(metric_data["four_stage"])
        
        print("\n" + "=" * 80)
        print("æ€»ä½“ç»Ÿè®¡")
        print("=" * 80)
        
        for metric_name, values in metrics_to_avg.items():
            two_avg = sum(values["two"]) / len(values["two"]) if values["two"] else 0
            three_avg = sum(values["three"]) / len(values["three"]) if values["three"] else 0
            four_avg = sum(values["four"]) / len(values["four"]) if values["four"] else 0
            
            print(f"\n{metric_name}:")
            print(f"  2 Stage å¹³å‡: {two_avg:.3f}")
            if values["three"]:
                print(f"  3 Stage å¹³å‡: {three_avg:.3f}")
            if values["four"]:
                print(f"  4 Stage å¹³å‡: {four_avg:.3f}")
            
            # æ‰¾å‡ºæœ€ä½³å¹³å‡å€¼
            avgs = {"2 Stage": two_avg}
            if values["three"]:
                avgs["3 Stage"] = three_avg
            if values["four"]:
                avgs["4 Stage"] = four_avg
            best_avg = max(avgs.items(), key=lambda x: x[1])[0] if avgs else "2 Stage"
            print(f"  æœ€ä½³å¹³å‡: {best_avg} ({max(avgs.values()):.3f})")




if __name__ == "__main__":
    main()

