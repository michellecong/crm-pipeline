#!/usr/bin/env python3
"""
æµ‹è¯•å®Œæ•´çš„CRM Pipelineæµç¨‹ï¼šä»æŠ“å–æ•°æ®åˆ°ç”Ÿæˆå†…å®¹
"""
import asyncio
import json
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from app.controllers.scraping_controller import get_scraping_controller
from app.services.data_aggregator import get_data_aggregator
from app.services.generator_service import get_generator_service
from app.services.data_store import get_data_store
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„CRM Pipelineæµç¨‹"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•å®Œæ•´çš„CRM Pipelineæµç¨‹")
    print("=" * 60)
    
    # Test company
    company_name = "Tesla"
    
    try:
        # Step 1: æŠ“å–æ•°æ®
        print(f"\nğŸ“¡ Step 1: æŠ“å– {company_name} çš„æ•°æ®...")
        controller = get_scraping_controller()
        
        scrape_result = await controller.scrape_company(
            company_name=company_name,
            include_news=True,
            include_case_studies=True,
            max_urls=5,  # é™åˆ¶URLæ•°é‡ä»¥åŠ å¿«æµ‹è¯•
            save_to_file=True  # ä¿å­˜åˆ°æ–‡ä»¶
        )
        
        print(f"âœ… æŠ“å–å®Œæˆ!")
        print(f"   - æ‰¾åˆ°URLæ•°é‡: {scrape_result['total_urls_found']}")
        print(f"   - æˆåŠŸæŠ“å–: {scrape_result['successful_scrapes']}")
        print(f"   - ä¿å­˜æ–‡ä»¶: {scrape_result.get('saved_filepath', 'N/A')}")
        
        # æ˜¾ç¤ºå†…å®¹å¤„ç†ç»Ÿè®¡
        content_processing = scrape_result.get('content_processing', {})
        print(f"   - å¤„ç†çš„å†…å®¹é¡¹: {content_processing.get('processed_items', 0)}/{content_processing.get('total_items', 0)}")
        
        # Step 2: éªŒè¯ä¿å­˜çš„æ•°æ®
        print(f"\nğŸ’¾ Step 2: éªŒè¯ä¿å­˜çš„æ•°æ®...")
        data_store = get_data_store()
        saved_data = data_store.load_latest_scraped_data(company_name)
        
        if saved_data:
            print(f"âœ… æˆåŠŸåŠ è½½ä¿å­˜çš„æ•°æ®")
            print(f"   - å…¬å¸åç§°: {saved_data['company_name']}")
            print(f"   - å®˜æ–¹ç½‘ç«™: {saved_data.get('official_website', 'N/A')}")
            print(f"   - å†…å®¹é¡¹æ•°é‡: {len(saved_data.get('scraped_content', []))}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤„ç†åçš„æ•°æ®
            processed_items = [item for item in saved_data.get('scraped_content', []) 
                             if 'processed_markdown' in item]
            print(f"   - å·²å¤„ç†çš„å†…å®¹é¡¹: {len(processed_items)}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå†…å®¹é¡¹çš„ç»Ÿè®¡
            if processed_items:
                first_item = processed_items[0]
                print(f"   - ç¬¬ä¸€ä¸ªå†…å®¹é¡¹:")
                print(f"     URL: {first_item.get('url', 'N/A')}")
                print(f"     ç±»å‹: {first_item.get('content_type', 'N/A')}")
                print(f"     åŸå§‹é•¿åº¦: {first_item.get('original_markdown_length', 0)}")
                print(f"     å¤„ç†åé•¿åº¦: {first_item.get('processed_markdown_length', 0)}")
                print(f"     å‹ç¼©æ¯”ä¾‹: {first_item.get('compression_ratio', 0):.2f}")
        else:
            print("âŒ æ— æ³•åŠ è½½ä¿å­˜çš„æ•°æ®")
            return
        
        # Step 3: å‡†å¤‡ä¸Šä¸‹æ–‡
        print(f"\nğŸ”§ Step 3: å‡†å¤‡ç”Ÿæˆä¸Šä¸‹æ–‡...")
        data_aggregator = get_data_aggregator()
        
        context = await data_aggregator.prepare_context(
            company_name=company_name,
            max_chars=8000,  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            include_news=True,
            include_case_studies=True,
            max_urls=5
        )
        
        print(f"âœ… ä¸Šä¸‹æ–‡å‡†å¤‡å®Œæˆ!")
        print(f"   - ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
        print(f"   - ä¸Šä¸‹æ–‡é¢„è§ˆ: {context[:200]}...")
        
        # Step 4: ç”Ÿæˆå†…å®¹
        print(f"\nğŸ¯ Step 4: ç”ŸæˆPersonaå†…å®¹...")
        generator_service = get_generator_service()
        
        generation_result = await generator_service.generate(
            generator_type="personas",
            company_name=company_name,
            generate_count=2,  # ç”Ÿæˆ2ä¸ªpersona
            max_context_chars=8000,
            include_news=True,
            include_case_studies=True,
            max_urls=5
        )
        
        print(f"âœ… å†…å®¹ç”Ÿæˆå®Œæˆ!")
        print(f"   - ç”Ÿæˆç»“æœ: {generation_result.get('success', False)}")
        
        if generation_result.get('success'):
            result_data = generation_result.get('result', {})
            personas = result_data.get('personas', [])
            print(f"   - ç”Ÿæˆçš„Personaæ•°é‡: {len(personas)}")
            print(f"   - ä¿å­˜æ–‡ä»¶: {generation_result.get('saved_filepath', 'N/A')}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªpersonaçš„æ¦‚è¦
            if personas:
                first_persona = personas[0]
                print(f"   - ç¬¬ä¸€ä¸ªPersona:")
                print(f"     åç§°: {first_persona.get('name', 'N/A')}")
                print(f"     èŒä½: {first_persona.get('title', 'N/A')}")
                print(f"     å±‚çº§: {first_persona.get('tier', 'N/A')}")
                print(f"     ç—›ç‚¹: {first_persona.get('pain_points', [])[:2]}...")  # åªæ˜¾ç¤ºå‰2ä¸ª
        
        # Step 5: æ˜¾ç¤ºå®Œæ•´æµç¨‹æ€»ç»“
        print(f"\nğŸ“Š Step 5: æµç¨‹æ€»ç»“")
        print("=" * 60)
        print(f"âœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸ!")
        print(f"   - å…¬å¸: {company_name}")
        print(f"   - æŠ“å–URL: {scrape_result['total_urls_found']}")
        print(f"   - æˆåŠŸæŠ“å–: {scrape_result['successful_scrapes']}")
        print(f"   - å†…å®¹å¤„ç†: {content_processing.get('processed_items', 0)} é¡¹")
        print(f"   - ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
        print(f"   - ç”ŸæˆPersona: {len(personas) if generation_result.get('success') else 0} ä¸ª")
        print(f"   - ä¿å­˜æ–‡ä»¶: {scrape_result.get('saved_filepath', 'N/A')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        logger.exception("Full pipeline test failed")
        return False


async def test_data_flow():
    """æµ‹è¯•æ•°æ®æµ"""
    print(f"\nğŸ” æµ‹è¯•æ•°æ®æµ...")
    
    company_name = "Tesla"
    data_store = get_data_store()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„æ•°æ®
    saved_data = data_store.load_latest_scraped_data(company_name)
    if not saved_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„æ•°æ®ï¼Œè¯·å…ˆè¿è¡ŒæŠ“å–æµ‹è¯•")
        return False
    
    print(f"âœ… æ‰¾åˆ°ä¿å­˜çš„æ•°æ®")
    
    # æ£€æŸ¥æ•°æ®ç»“æ„
    scraped_content = saved_data.get('scraped_content', [])
    processed_items = [item for item in scraped_content if 'processed_markdown' in item]
    
    print(f"   - æ€»å†…å®¹é¡¹: {len(scraped_content)}")
    print(f"   - å·²å¤„ç†é¡¹: {len(processed_items)}")
    
    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
    if processed_items:
        total_original = sum(item.get('original_markdown_length', 0) for item in processed_items)
        total_processed = sum(item.get('processed_markdown_length', 0) for item in processed_items)
        avg_compression = total_processed / total_original if total_original > 0 else 0
        
        print(f"   - åŸå§‹æ€»é•¿åº¦: {total_original}")
        print(f"   - å¤„ç†åæ€»é•¿åº¦: {total_processed}")
        print(f"   - å¹³å‡å‹ç¼©æ¯”ä¾‹: {avg_compression:.2f}")
    
    return True


if __name__ == "__main__":
    print("ğŸ§ª CRM Pipeline å®Œæ•´æµç¨‹æµ‹è¯•")
    print("=" * 60)
    
    async def main():
        # æµ‹è¯•å®Œæ•´æµç¨‹
        success = await test_full_pipeline()
        
        if success:
            # æµ‹è¯•æ•°æ®æµ
            await test_data_flow()
        
        print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())


